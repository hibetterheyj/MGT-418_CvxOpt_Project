{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ad60ca",
   "metadata": {},
   "source": [
    "# Radar Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5145a2",
   "metadata": {},
   "source": [
    "## A Radar Classification Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a41617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from cvxpy.atoms.affine.wraps import psd_wrap\n",
    "from read_data import *\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%       MGT - 418         %%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Convex Optimization - Project 2          %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%             2021-2022 Fall                    %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Learning the Kernel Function             %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c9907",
   "metadata": {},
   "source": [
    "### (a) Read & Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc34d0",
   "metadata": {},
   "source": [
    "**(5 points)** Read the data file ionosphere.data into memory by using the scriptsreaddata.pyorreaddata.m.Use the code skeletonsmain.ipnyb or main.m to randomly select 80% of the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729b41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = prepare_ionosphere_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b795ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "\n",
    "#%% split train and test dataset\n",
    "@contextlib.contextmanager\n",
    "def temp_seed(seed):\n",
    "    state = np.random.get_state()\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        np.random.set_state(state)\n",
    "\n",
    "\n",
    "def train_test_split(data, labels, train_size=0.8, random_seed=0):\n",
    "    with temp_seed(random_seed):\n",
    "        indices = np.random.permutation(data.shape[0])\n",
    "        train_num = round(np.shape(data)[0] * train_size)\n",
    "        train_idx, test_idx = indices[:train_num], indices[train_num:]\n",
    "        # data_train, data_test, labels_train, labels_test\n",
    "        return (\n",
    "            data[train_idx, :],\n",
    "            data[test_idx, :],\n",
    "            labels[train_idx],\n",
    "            labels[test_idx],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cabc1712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0\n",
      "[[1.0 0.97588 -0.10602 0.94601 -0.208 0.92806 -0.2835 0.85996 -0.27342\n",
      "  0.79766 -0.47929 0.78225 -0.50764 0.74628 -0.61436 0.57945 -0.68086\n",
      "  0.37852 -0.73641 0.36324 -0.76562 0.31898 -0.79753 0.22792 -0.81634\n",
      "  0.13659 -0.8251 0.04606 -0.82395 -0.04262 -0.81318 -0.13832 -0.80975]]\n",
      "#1\n",
      "[[1.0 -0.205 0.2875 0.23 0.1 0.2825 0.3175 0.3225 0.35 0.36285 -0.34617\n",
      "  0.0925 0.275 -0.095 0.21 -0.0875 0.235 -0.34187 0.31408 -0.48 -0.08\n",
      "  0.29908 0.33176 -0.58 -0.24 0.3219 -0.28475 -0.47 0.185 -0.27104\n",
      "  -0.31228 0.40445 0.0305]]\n",
      "#2\n",
      "[[1.0 1.0 0.5782 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0\n",
      "  1.0 -1.0 1.0 -1.0 1.0 -0.62796 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0\n",
      "  -1.0 1.0 -1.0]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"#{}\".format(i))\n",
    "    # data_train, data_test, labels_train, labels_test\n",
    "    data_train, _, _, _ = train_test_split(data, labels, train_size=0.8, random_seed=i)\n",
    "    print(data_train[:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91059b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(351, 35)\n",
      "Unique values in 2nd column: [0]\n",
      "(351, 34)\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "# just testing\n",
    "df = pd.read_csv('ionosphere.data', sep=\",\", header=None)\n",
    "data_array = np.array(df)\n",
    "print(np.shape(data_array))\n",
    "# delete the second row\n",
    "print(\"Unique values in 2nd column:\", np.unique(data_array[:,1]))\n",
    "data_array = np.delete(data_array, 1, 1)\n",
    "print(np.shape(data_array))\n",
    "print(round(np.shape(data_array)[0]*0.8))\n",
    "labels = data_array[:, -1]\n",
    "labels[labels == 'g'] = -1\n",
    "labels[labels == 'b'] = 1\n",
    "data_array = data_array[:, :-1]\n",
    "data_normalized = data_array / data_array.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a016f",
   "metadata": {},
   "source": [
    "### (b) Define kernel function & Solve QCQP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f054940",
   "metadata": {},
   "source": [
    "**(15 points)** Define polynomial, Gaussian, and linear kernel function, and construct the kernel matrices $\\hat{K}^l,\\ l = 1,2,3$, for all training samples\n",
    "Solve the QCQP in (5) for $\\rho = 2$, $p = 2$, $\\sigma = 2$ and $c = \\sum_{l=1}^3 \\mathrm{tr}(\\hat{K}^l) $ with CVXPY and MOSEK in Python or with YALMIP and GUROBI in MATLAB, and record the optimal dualvariables $\\mu_1^*$, $\\mu_2^*$, and $\\mu_3^*$. Use the code skeletons `kernel_learning` (in `main.ipynb`) or `kernel_learning.m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba56978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_trans(x_mat, y_mat, kernel, sigma=0.5, degree=2):\n",
    "    m = np.shape(x_mat)[0]\n",
    "    k_mat = np.mat(np.zeros((m, 1)))\n",
    "    if kernel == 'lin':  # 线性核函数\n",
    "        k_mat = x_mat @ y_mat.T\n",
    "\n",
    "    elif kernel == 'rbf':  # 高斯核\n",
    "        for j in range(m):\n",
    "            deltaRow = x_mat[j, :] - y_mat\n",
    "            k_mat[j] = deltaRow @ deltaRow.T\n",
    "        k_mat = np.exp(-k_mat / (2 * sigma))\n",
    "\n",
    "    elif kernel == 'poly':\n",
    "        k_mat = 1 + x_mat @ y_mat.T\n",
    "        for j in range(m):\n",
    "            k_mat[j] = k_mat[j] ** degree\n",
    "\n",
    "    else:\n",
    "        raise NameError('Not implemented')\n",
    "    return k_mat\n",
    "\n",
    "\n",
    "def compute_kmat(x_mat, y_mat, kernel, sigma=0.5, degree=2):\n",
    "    n_in, n_out = np.shape(x_mat)[0], np.shape(y_mat)[0]\n",
    "    kmat = np.mat(np.zeros((n_in, n_out)))\n",
    "    for idx in range(n_out):\n",
    "        kmat[:, idx] = kernel_trans(\n",
    "            x_mat,\n",
    "            y_mat[idx, :].reshape((1, -1)),\n",
    "            kernel=kernel,\n",
    "            sigma=sigma,\n",
    "            degree=degree,\n",
    "        )\n",
    "    return kmat\n",
    "\n",
    "\n",
    "def compute_all_kmat(x_mat, y_mat):\n",
    "    \"\"\"compute all three kernel matrix\"\"\"\n",
    "    kernels = [\"poly\", \"rbf\", \"lin\"]\n",
    "    kmats = []\n",
    "    n_in, n_out = np.shape(x_mat)[0], np.shape(y_mat)[0]\n",
    "\n",
    "    k_poly = compute_kmat(x_mat, y_mat, 'poly', sigma=0.5, degree=2)\n",
    "    k_rbf = compute_kmat(x_mat, y_mat, 'rbf', sigma=0.5, degree=2)\n",
    "    k_lin = compute_kmat(x_mat, y_mat, 'lin', sigma=0.5, degree=2)\n",
    "    \n",
    "    return (k_poly, k_rbf, k_lin)\n",
    "\n",
    "\n",
    "def compute_gmat(k_mat, y_vec):\n",
    "    gmat = np.zeros((k_mat.shape[0], k_mat.shape[1]))\n",
    "    for i in range(k_mat.shape[0]):\n",
    "        for j in range(k_mat.shape[1]):\n",
    "            gmat[i, j] = k_mat[i, j] * y_vec[i] * y_vec[j]\n",
    "    return gmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfbe76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_learning(K1, K2, K3, y_tr, rho=2):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        kmats = [K1, K2, K3i s list of (n_tr, n_tr) matrix.\n",
    "        y_tr is (n_tr,) array\n",
    "    Output:\n",
    "        mu_opt1, mu_opt2, mu_opt3, lambda_.value, b_opt\n",
    "    Kernel learning for soft margin SVM.\n",
    "    Implementation of problem (5)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when\n",
    "    it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "\n",
    "    r1 = np.trace(K1)\n",
    "    r2 = np.trace(K2)\n",
    "    r3 = np.trace(K3)\n",
    "    \n",
    "    G1 = compute_gmat(K1, y_tr)\n",
    "    G2 = compute_gmat(K2, y_tr)\n",
    "    G3 = compute_gmat(K3, y_tr)\n",
    "    \n",
    "    c = np.sum([r1, r2, r3])\n",
    "\n",
    "    n_tr = len(y_tr)\n",
    "    lambda_ = cp.Variable(n_tr)\n",
    "    z = cp.Variable(1)\n",
    "\n",
    "    obj = cp.Maximize(cp.sum(lambda_) - c * z)\n",
    "    \n",
    "    cons = []\n",
    "    cons.append(z * r1 >= (1 / (2 * rho)) * cp.quad_form(lambda_, psd_wrap(G1)))\n",
    "    cons.append(z * r2 >= (1 / (2 * rho)) * cp.quad_form(lambda_, psd_wrap(G2)))\n",
    "    cons.append(z * r3 >= (1 / (2 * rho)) * cp.quad_form(lambda_, psd_wrap(G3)))\n",
    "    cons.append(lambda_.T @ y_tr == 0)  # lambda_ * y_tr == 0\n",
    "    cons.extend([lambda_ >= 0, lambda_ <= 1])\n",
    "\n",
    "    prob = cp.Problem(obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "\n",
    "    # mu_opt_l (l=1,2,3) denote the optimal dual value of the constraint\n",
    "    mu_opt1 = cons[0].dual_value\n",
    "    mu_opt2 = cons[1].dual_value\n",
    "    mu_opt3 = cons[2].dual_value\n",
    "\n",
    "    # from 4(c) b_opt is the dual variable of the constraint `lambda_ @ y_tr == 0`\n",
    "    b_opt = cons[3].dual_value\n",
    "\n",
    "    return mu_opt1, mu_opt2, mu_opt3, lambda_.value, b_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4b57f",
   "metadata": {},
   "source": [
    "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
    "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
    "    Use ``multiply`` for elementwise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "671cc261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_opt1 = [0.12139619]\n",
      "mu_opt2 = [234.09133771]\n",
      "mu_opt3 = [0.90737479]\n",
      "b_opt = 0.5762281224579241\n"
     ]
    }
   ],
   "source": [
    "def main_q4_b():\n",
    "    data, labels = prepare_ionosphere_dataset()\n",
    "\n",
    "    x_tr, x_te, y_tr, y_te = train_test_split(data, labels, train_size=0.8)\n",
    "\n",
    "    (k_poly, k_rbf, k_lin) = compute_all_kmat(x_tr, x_tr)\n",
    "    \n",
    "    rho = 2\n",
    "    res = kernel_learning(k_poly, k_rbf, k_lin, y_tr, rho=rho)\n",
    "    \n",
    "    print(\"mu_opt1 =\", res[0])\n",
    "    print(\"mu_opt2 =\", res[1])\n",
    "    print(\"mu_opt3 =\", res[2])\n",
    "    print(\"b_opt =\", res[4])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main_q4_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff54f9",
   "metadata": {},
   "source": [
    "### (c) Apply kernel trick for SVM prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c8a42",
   "metadata": {},
   "source": [
    "**(10 points)** Use the code skeletons `SVM_predict`(in `main.ipynb`) or `SVM_predict.m`.\n",
    "\n",
    "> The size of the dual QP is independent of the feature\n",
    "dimension D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2a7cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_predict(kernel, y_tr, y_te, lambda_opt, b_opt, rho):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        kernel: kernel matrix\n",
    "    Predict function for kernel SVM. \n",
    "    See lecture slide 183.\n",
    "    \"\"\"\n",
    "    n_te = len(y_te)\n",
    "    n_tr = len(y_tr)\n",
    "    \n",
    "    # wx = \\sum_{i=1}^{m} \\lambda_i y_i k_mat\n",
    "    result = b_opt\n",
    "    for idx, (lambda_i, y_i) in enumerate(zip(lambda_opt, y_tr)):\n",
    "        result += 1/rho * lambda_i * y_i * kernel[idx,:]\n",
    "    pred_y_te = np.sign(result)\n",
    "\n",
    "    acc = np.sum(np.equal(y_te, pred_y_te))/n_te\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b94f6bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12139619] [234.09133771] [0.90737479]\n",
      "Current accuracy is  0.9571428571428572\n"
     ]
    }
   ],
   "source": [
    "def main_q4_c():\n",
    "    data, labels = prepare_ionosphere_dataset()\n",
    "    \n",
    "    x_tr, x_te, y_tr, y_te = train_test_split(data, labels, train_size=0.8, random_seed=0)\n",
    "    \n",
    "    (k_poly, k_rbf, k_lin) = compute_all_kmat(x_tr, x_tr)\n",
    "    \n",
    "    (mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt) = kernel_learning(k_poly, k_rbf, k_lin, y_tr, rho=2)\n",
    "    \n",
    "    kxx_prime = compute_all_kmat(x_tr, x_te)\n",
    "    \n",
    "    print(mu_opt1, mu_opt2, mu_opt3)\n",
    "    \n",
    "    # for kxx_ in kxx_prime:\n",
    "    #     print(kxx_[:5,:5])\n",
    "    \n",
    "    kxx_prime_weighted = mu_opt1[0] * kxx_prime[0] + mu_opt2[0] * kxx_prime[1] + mu_opt3[0] * kxx_prime[2]\n",
    "    \n",
    "    acc = svm_predict(kxx_prime_weighted, y_tr, y_te, lambda_opt, b_opt, rho=2)\n",
    "    \n",
    "    print(\"Current accuracy is \", acc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_q4_c()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028bf40",
   "metadata": {},
   "source": [
    "## B Repeat experiments & Solve dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd9a3b",
   "metadata": {},
   "source": [
    "5. **(5 points)** Repeat the steps 4(a)–(c) 100 times with different seeds for the random partition of the data intotraining and test sets, and report the average test accuracy (correct classification rate) to Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc4049",
   "metadata": {},
   "source": [
    "6. **(10  points)** For  each  of  the  100  training  and  test  sets  constructed  in  5.,  solve  (2)  using  the  kernels  functions $\\hat{k}^1$, $\\hat{k}^2$, and $\\hat{k}^3$, respectively, and report the average test accuracies in Table 1. Use the code skeletons `SVM_predict`(in `main.ipynb`) or `SVM_predict.m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "335fe58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_fit(kernel, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        kernel is kernel matrix\n",
    "    Dual of soft-margin SVM problem (2)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "    n_tr = len(y_tr)\n",
    "    # todo: check compuation of G\n",
    "    G = compute_gmat(kernel, y_tr)\n",
    "    lambda_ = cp.Variable(n_tr)\n",
    "    dual_obj = cp.Maximize(cp.sum(lambda_) - 1 / (2 * rho) * cp.quad_form(lambda_, psd_wrap(G)))\n",
    "    cons = []\n",
    "    cons.append(lambda_.T @ y_tr == 0)  # lambda_ * y_tr == 0\n",
    "    cons.extend([lambda_ >= 0, lambda_ <= 1])\n",
    "\n",
    "    prob = cp.Problem(dual_obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "    lambda_opt = lambda_.value\n",
    "    b_opt =  cons[0].dual_value\n",
    "    return lambda_opt, b_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d13aa9",
   "metadata": {},
   "source": [
    "- old split method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b6e5e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration-->10\n",
      "Iteration-->20\n",
      "Iteration-->30\n",
      "Iteration-->40\n",
      "Iteration-->50\n",
      "Iteration-->60\n",
      "Iteration-->70\n",
      "Iteration-->80\n",
      "Iteration-->90\n",
      "Iteration-->100\n",
      "Average dual accuracy with optimal kernel is 0.8714285714285716\n",
      "Average dual accuracy with polynomial kernel is 0.9285714285714287\n",
      "Average dual accuracy with gaussian kernel is 0.9714285714285715\n",
      "Average dual accuracy with linear kernel is 0.885714285714286\n"
     ]
    }
   ],
   "source": [
    "acc_opt_kernel = []    \n",
    "acc_poly_kernel = []    \n",
    "acc_gauss_kernel = []    \n",
    "acc_linear_kernel = []    \n",
    "rho = 0.01\n",
    "data, labels = prepare_ionosphere_dataset()\n",
    "for iters in range(100): \n",
    "    \n",
    "    ## Please do not change the random seed.\n",
    "    np.random.seed(iters)\n",
    "    ### Training-test split\n",
    "    msk = np.random.rand(data.shape[0]) <= 0.8\n",
    "    x_tr = data[msk,:]\n",
    "    x_te = data[~msk,:]\n",
    "    y_tr = labels[msk]\n",
    "    y_te = labels[~msk]\n",
    "    #x_tr, x_te, y_tr, y_te = train_test_split(data, labels, train_size=0.8)\n",
    " \n",
    "    n_tr = y_tr.shape[0]\n",
    "    n_te = y_te.shape[0]\n",
    "    \n",
    "    x_all = np.vstack([x_tr, x_te])\n",
    "    n_all = x_all.shape[0]\n",
    "\n",
    "    ## Prepare the initial choice of kernels \n",
    "    # It is recommended to prepare the kernels for all the training and the test data\n",
    "    # Then, the kernel size will be (n_tr + n_te)x(n_tr + n_te).\n",
    "    # Use only the training block (like K1[0:n_tr, 0:n_tr] ) to learn the classifier \n",
    "    # (for the functions svm_fit and kernel_learning).\n",
    "    # When predicting you may use the whole kernel as it is. \n",
    "    K1 = compute_kmat(x_tr, x_tr, \"poly\", sigma=0.5, degree=2)\n",
    "    K2 = compute_kmat(x_tr, x_tr, \"rbf\", sigma=0.5, degree=2)\n",
    "    K3 = compute_kmat(x_tr, x_tr, \"lin\", sigma=0.5, degree=2)\n",
    "    \n",
    "    kxx_prime1 = compute_kmat(x_tr, x_te, \"poly\", sigma=0.5, degree=2)\n",
    "    kxx_prime2 = compute_kmat(x_tr, x_te, \"rbf\", sigma=0.5, degree=2)\n",
    "    kxx_prime3 = compute_kmat(x_tr, x_te, \"lin\", sigma=0.5, degree=2)\n",
    "\n",
    "    mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt = kernel_learning(K1, K2, K3, y_tr, rho=2)\n",
    "    opt_kernel = (mu_opt1[0] * kxx_prime1\n",
    "                  + mu_opt2[0] * kxx_prime2\n",
    "                  + mu_opt3[0] * kxx_prime3)                 \n",
    "    acc_opt_kernel.append(svm_predict(opt_kernel, y_tr, y_te, lambda_opt, b_opt, rho))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(K1, y_tr, rho)\n",
    "    acc_poly_kernel.append(svm_predict(kxx_prime1, y_tr, y_te, lambda_opt, b_opt, rho))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(K2, y_tr, rho)\n",
    "    \n",
    "    acc_gauss_kernel.append(svm_predict(kxx_prime2, y_tr, y_te, lambda_opt, b_opt, rho))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(K3, y_tr, rho)\n",
    "    acc_linear_kernel.append(svm_predict(kxx_prime3, y_tr, y_te, lambda_opt, b_opt, rho))\n",
    "    if (iters+1) % 10 == 0:\n",
    "        print('Iteration-->' + str(iters+1))\n",
    "print('Average dual accuracy with optimal kernel is ' + str(np.mean(acc_opt_kernel)))\n",
    "print('Average dual accuracy with polynomial kernel is ' + str(np.mean(acc_poly_kernel)))\n",
    "print('Average dual accuracy with gaussian kernel is ' + str(np.mean(acc_gauss_kernel)))\n",
    "print('Average dual accuracy with linear kernel is ' + str(np.mean(acc_linear_kernel)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb2215",
   "metadata": {},
   "source": [
    "- mask method cannot generate correct 4-1 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "522afbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "78\n",
      "62\n",
      "62\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "data, labels = prepare_ionosphere_dataset()\n",
    "## Please do not change the random seed.\n",
    "for iters in range(5): \n",
    "    np.random.seed(iters)\n",
    "    ### Training-test split\n",
    "    msk = np.random.rand(data.shape[0]) <= 0.8\n",
    "    x_tr = data[msk,:]\n",
    "    x_te = data[~msk,:]\n",
    "    y_tr = labels[msk]\n",
    "    y_te = labels[~msk]\n",
    "    n_te = x_te.shape[0]\n",
    "    print(n_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "623eee54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration-->10\n",
      "Iteration-->20\n",
      "Iteration-->30\n",
      "Iteration-->40\n",
      "Iteration-->50\n",
      "Iteration-->60\n",
      "Iteration-->70\n",
      "Iteration-->80\n",
      "Iteration-->90\n",
      "Iteration-->100\n",
      "Average dual accuracy with optimal kernel is 0.792918644523617\n",
      "Average dual accuracy with polynomial kernel is 0.8811118434821189\n",
      "Average dual accuracy with gaussian kernel is 0.9242464280296088\n",
      "Average dual accuracy with linear kernel is 0.8702032889741905\n"
     ]
    }
   ],
   "source": [
    "acc_opt_kernel = []    \n",
    "acc_poly_kernel = []    \n",
    "acc_gauss_kernel = []    \n",
    "acc_linear_kernel = []    \n",
    "rho = 0.01\n",
    "data, labels = prepare_ionosphere_dataset()\n",
    "for iters in range(100): \n",
    "    \n",
    "    ## Please do not change the random seed.\n",
    "    np.random.seed(iters)\n",
    "    ### Training-test split\n",
    "    msk = np.random.rand(data.shape[0]) <= 0.8\n",
    "    x_tr = data[msk,:]\n",
    "    x_te = data[~msk,:]\n",
    "    y_tr = labels[msk]\n",
    "    y_te = labels[~msk]\n",
    " \n",
    "    n_tr = y_tr.shape[0]\n",
    "    n_te = y_te.shape[0]\n",
    "    \n",
    "    x_all = np.vstack([x_tr, x_te])\n",
    "    n_all = x_all.shape[0]\n",
    "\n",
    "    ## Prepare the initial choice of kernels \n",
    "    # It is recommended to prepare the kernels for all the training and the test data\n",
    "    # Then, the kernel size will be (n_tr + n_te)x(n_tr + n_te).\n",
    "    # Use only the training block (like K1[0:n_tr, 0:n_tr] ) to learn the classifier \n",
    "    # (for the functions svm_fit and kernel_learning).\n",
    "    # When predicting you may use the whole kernel as it is. \n",
    "    K1 = compute_kmat(x_tr, x_tr, \"poly\", sigma=0.5, degree=2)\n",
    "    K2 = compute_kmat(x_tr, x_tr, \"rbf\", sigma=0.5, degree=2)\n",
    "    K3 = compute_kmat(x_tr, x_tr, \"lin\", sigma=0.5, degree=2)\n",
    "    \n",
    "    kxx_prime1 = compute_kmat(x_tr, x_te, \"poly\", sigma=0.5, degree=2)\n",
    "    kxx_prime2 = compute_kmat(x_tr, x_te, \"rbf\", sigma=0.5, degree=2)\n",
    "    kxx_prime3 = compute_kmat(x_tr, x_te, \"lin\", sigma=0.5, degree=2)\n",
    "\n",
    "    mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt = kernel_learning(K1, K2, K3, y_tr, rho=2)\n",
    "    opt_kernel = (mu_opt1[0] * kxx_prime1\n",
    "                  + mu_opt2[0] * kxx_prime2\n",
    "                  + mu_opt3[0] * kxx_prime3)                 \n",
    "    acc_opt_kernel.append(svm_predict(opt_kernel, y_tr, y_te, lambda_opt, b_opt, rho))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(K1, y_tr, rho)\n",
    "    acc_poly_kernel.append(svm_predict(kxx_prime1, y_tr, y_te, lambda_opt, b_opt, rho))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(K2, y_tr, rho)\n",
    "    \n",
    "    acc_gauss_kernel.append(svm_predict(kxx_prime2, y_tr, y_te, lambda_opt, b_opt, rho))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(K3, y_tr, rho)\n",
    "    acc_linear_kernel.append(svm_predict(kxx_prime3, y_tr, y_te, lambda_opt, b_opt, rho))\n",
    "    if (iters+1) % 10 == 0:\n",
    "        print('Iteration-->' + str(iters+1))\n",
    "print('Average dual accuracy with optimal kernel is ' + str(np.mean(acc_opt_kernel)))\n",
    "print('Average dual accuracy with polynomial kernel is ' + str(np.mean(acc_poly_kernel)))\n",
    "print('Average dual accuracy with gaussian kernel is ' + str(np.mean(acc_gauss_kernel)))\n",
    "print('Average dual accuracy with linear kernel is ' + str(np.mean(acc_linear_kernel)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff15c8",
   "metadata": {},
   "source": [
    "```\n",
    "Average dual accuracy with optimal kernel is 0.8714285714285716\n",
    "Average dual accuracy with polynomial kernel is 0.9285714285714287\n",
    "Average dual accuracy with gaussian kernel is 0.9714285714285715\n",
    "Average dual accuracy with linear kernel is 0.885714285714286\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0a2cc",
   "metadata": {},
   "source": [
    "| Kernel function                  | Average accuracy   |\n",
    "| :------------------------------- | :----------------- |\n",
    "| $\\hat{k}^1$                      | 0.9285714285714287 |\n",
    "| $\\hat{k}^2$                      | 0.9714285714285715 |\n",
    "| $\\hat{k}^3$                      | 0.885714285714286  |\n",
    "| $\\sum_{l=1}^3 \\mu^*_i \\hat{k}^l$ | 0.8714285714285716 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0032d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
