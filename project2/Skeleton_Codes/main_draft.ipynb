{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ad60ca",
   "metadata": {},
   "source": [
    "# Radar Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5145a2",
   "metadata": {},
   "source": [
    "## A Radar Classification Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a41617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from cvxpy.atoms.affine.wraps import psd_wrap\n",
    "from read_data import *\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%       MGT - 418         %%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Convex Optimization - Project 2          %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%             2021-2022 Fall                    %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Learning the Kernel Function             %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c9907",
   "metadata": {},
   "source": [
    "### (a) Read & Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc34d0",
   "metadata": {},
   "source": [
    "**(5 points)** Read the data file ionosphere.data into memory by using the scriptsreaddata.pyorreaddata.m.Use the code skeletonsmain.ipnyb or main.m to randomly select 80% of the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729b41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = prepare_ionosphere_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b795ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_data import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cabc1712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0\n",
      "[[1.0 0.97588 -0.10602 0.94601 -0.208 0.92806 -0.2835 0.85996 -0.27342\n",
      "  0.79766 -0.47929 0.78225 -0.50764 0.74628 -0.61436 0.57945 -0.68086\n",
      "  0.37852 -0.73641 0.36324 -0.76562 0.31898 -0.79753 0.22792 -0.81634\n",
      "  0.13659 -0.8251 0.04606 -0.82395 -0.04262 -0.81318 -0.13832 -0.80975]]\n",
      "#1\n",
      "[[1.0 -0.205 0.2875 0.23 0.1 0.2825 0.3175 0.3225 0.35 0.36285 -0.34617\n",
      "  0.0925 0.275 -0.095 0.21 -0.0875 0.235 -0.34187 0.31408 -0.48 -0.08\n",
      "  0.29908 0.33176 -0.58 -0.24 0.3219 -0.28475 -0.47 0.185 -0.27104\n",
      "  -0.31228 0.40445 0.0305]]\n",
      "#2\n",
      "[[1.0 1.0 0.5782 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0\n",
      "  1.0 -1.0 1.0 -1.0 1.0 -0.62796 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0\n",
      "  -1.0 1.0 -1.0]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"#{}\".format(i))\n",
    "    # data_train, data_test, labels_train, labels_test\n",
    "    data_train, _, _, _ = train_test_split(data, labels, train_size=0.8, random_seed=i)\n",
    "    print(data_train[:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91059b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(351, 35)\n",
      "Unique values in 2nd column: [0]\n",
      "(351, 34)\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "# just testing\n",
    "df = pd.read_csv('ionosphere.data', sep=\",\", header=None)\n",
    "data_array = np.array(df)\n",
    "print(np.shape(data_array))\n",
    "# delete the second row\n",
    "print(\"Unique values in 2nd column:\", np.unique(data_array[:,1]))\n",
    "data_array = np.delete(data_array, 1, 1)\n",
    "print(np.shape(data_array))\n",
    "print(round(np.shape(data_array)[0]*0.8))\n",
    "labels = data_array[:, -1]\n",
    "labels[labels == 'g'] = -1\n",
    "labels[labels == 'b'] = 1\n",
    "data_array = data_array[:, :-1]\n",
    "data_normalized = data_array / data_array.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a016f",
   "metadata": {},
   "source": [
    "### (b) Define kernel function & Solve QCQP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f054940",
   "metadata": {},
   "source": [
    "**(15 points)** Define polynomial, Gaussian, and linear kernel function, and construct the kernel matrices $\\hat{K}^l,\\ l = 1,2,3$, for all training samples\n",
    "Solve the QCQP in (5) for $\\rho = 2$, $p = 2$, $\\sigma = 2$ and $c = \\sum_{l=1}^3 \\mathrm{tr}(\\hat{K}^l) $ with CVXPY and MOSEK in Python or with YALMIP and GUROBI in MATLAB, and record the optimal dualvariables $\\mu_1^*$, $\\mu_2^*$, and $\\mu_3^*$. Use the code skeletons `kernel_learning` (in `main.ipynb`) or `kernel_learning.m`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65415e7",
   "metadata": {},
   "source": [
    "### new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba56978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_trans(x_mat, y_mat, kernel, sigma=0.5, degree=2):\n",
    "    m = np.shape(x_mat)[0]\n",
    "    k_mat = np.mat(np.zeros((m, 1)))\n",
    "    if kernel == 'lin':  # 线性核函数\n",
    "        k_mat = x_mat @ y_mat.T\n",
    "\n",
    "    elif kernel == 'rbf':  # 高斯核\n",
    "        for j in range(m):\n",
    "            deltaRow = x_mat[j, :] - y_mat\n",
    "            k_mat[j] = deltaRow @ deltaRow.T\n",
    "        k_mat = np.exp(-k_mat / (2 * sigma))\n",
    "\n",
    "    elif kernel == 'poly':\n",
    "        k_mat = 1 + x_mat @ y_mat.T\n",
    "        for j in range(m):\n",
    "            k_mat[j] = k_mat[j] ** degree\n",
    "\n",
    "    else:\n",
    "        raise NameError('Not implemented')\n",
    "    return k_mat\n",
    "\n",
    "\n",
    "def compute_kmat(x_mat, y_mat, kernel, sigma=0.5, degree=2):\n",
    "    n_in, n_out = np.shape(x_mat)[0], np.shape(ymat)[0]\n",
    "    kmat = np.mat(np.zeros((n_in, n_out)))\n",
    "    for idx in range(n_out):\n",
    "        kmat[:, idx] = kernel_trans(\n",
    "            x_mat,\n",
    "            y_mat[idx, :].reshape((1, -1)),\n",
    "            kernel=kernel,\n",
    "            sigma=sigma,\n",
    "            degree=degree,\n",
    "        )\n",
    "    return kmat\n",
    "\n",
    "\n",
    "def compute_all_kmat(x_mat, y_mat):\n",
    "    \"\"\"compute all three kernel matrix\"\"\"\n",
    "    kernels = [\"poly\", \"rbf\", \"lin\"]\n",
    "    kmats = []\n",
    "    n_in, n_out = np.shape(x_mat)[0], np.shape(y_mat)[0]\n",
    "\n",
    "    for k in kernels:\n",
    "        kmat = np.mat(np.zeros((n_in, n_out)))\n",
    "        for idx in range(n_out):\n",
    "            kmat[:, idx] = kernel_trans(x_mat, y_mat[idx, :].reshape((1, -1)), kernel=k)\n",
    "        kmats.append(kmat)\n",
    "    return kmats\n",
    "\n",
    "\n",
    "def compute_gmat(k_mat, y_vec):\n",
    "    n_tr = len(y_vec)\n",
    "    gmat = np.zeros((n_tr, n_tr))\n",
    "    for i in range(n_tr):\n",
    "        for j in range(n_tr):\n",
    "            gmat[i, j] = k_mat[i, j] * y_vec[i] * y_vec[j]\n",
    "    return gmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a91bfc",
   "metadata": {},
   "source": [
    "```\n",
    "for con in enumerate(cons):\n",
    "    print(con)\n",
    "    \n",
    "(0, Inequality(Expression(CONVEX, NONNEGATIVE, (1, 1))))\n",
    "(1, Equality(Expression(AFFINE, UNKNOWN, ()), Constant(CONSTANT, ZERO, ())))\n",
    "(2, Inequality(Constant(CONSTANT, ZERO, ())))\n",
    "(3, Inequality(Variable((281,))))\n",
    "(4, Inequality(Expression(CONVEX, NONNEGATIVE, (1, 1))))\n",
    "(5, Equality(Expression(AFFINE, UNKNOWN, ()), Constant(CONSTANT, ZERO, ())))\n",
    "(6, Inequality(Constant(CONSTANT, ZERO, ())))\n",
    "(7, Inequality(Variable((281,))))\n",
    "(8, Inequality(Expression(CONVEX, NONNEGATIVE, (1, 1))))\n",
    "(9, Equality(Expression(AFFINE, UNKNOWN, ()), Constant(CONSTANT, ZERO, ())))\n",
    "(10, Inequality(Constant(CONSTANT, ZERO, ())))\n",
    "(11, Inequality(Variable((281,))))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfbe76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kernel_learning(K1, K2, K3, y_tr, rho)\n",
    "def kernel_learning(k_mats, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        Ks is list of (n_tr, n_tr) matrix.\n",
    "        y_tr is (n_tr,) array\n",
    "    Output:\n",
    "        mu_opt1, mu_opt2, mu_opt3, lambda_.value, b_opt\n",
    "    Kernel learning for soft margin SVM.\n",
    "    Implementation of problem (5)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when\n",
    "    it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "\n",
    "    # r1 = np.trace(K1)\n",
    "    # r2 = np.trace(K2)\n",
    "    # r3 = np.trace(K3)\n",
    "    # r_l = [r1, r2, r3]\n",
    "    r_l = []\n",
    "    gmats = []\n",
    "    for i in range(3):\n",
    "        r_l.append(np.trace(k_mats[i]))\n",
    "        gmat = compute_gmat(k_mats[i], y_tr)\n",
    "        gmats.append(gmat)\n",
    "    c = np.sum(r_l)\n",
    "\n",
    "    n_tr = len(y_tr)\n",
    "    lambda_ = cp.Variable(n_tr)\n",
    "    z = cp.Variable(1)\n",
    "\n",
    "    obj = cp.Maximize(cp.sum(lambda_) - c * z)\n",
    "    cons = []\n",
    "    for l in range(3):\n",
    "        # Exception: Invalid dimensions for arguments.\n",
    "        cons.append(\n",
    "            z * r_l[i] >= 1 / (2 * rho) * cp.quad_form(lambda_, psd_wrap(gmats[i]))\n",
    "        )  # G1\n",
    "        cons.append(lambda_ @ y_tr == 0)  # lambda_ * y_tr == 0\n",
    "        cons.extend([lambda_ >= 0, lambda_ <= 1])\n",
    "\n",
    "    prob = cp.Problem(obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "    # print(\"lambda_opt =\", np.shape(lambda_.value))\n",
    "\n",
    "    # mu_opt_l (l=1,2,3) denote the optimal dual value of the constraint\n",
    "    mu_opt1 = cons[0].dual_value\n",
    "    mu_opt2 = cons[4].dual_value\n",
    "    mu_opt3 = cons[8].dual_value\n",
    "    print(\"mu_opt1 =\", mu_opt1)\n",
    "    print(\"mu_opt2 =\", mu_opt2)\n",
    "    print(\"mu_opt3 =\", mu_opt3)\n",
    "\n",
    "    # from 4(c) b_opt is the dual variable of the constraint `lambda_ @ y_tr == 0`\n",
    "    b_opt = np.max([cons[1].dual_value, cons[5].dual_value, cons[9].dual_value])\n",
    "\n",
    "    # for idx in range(12):\n",
    "    #     print(idx, cons[idx].dual_value)\n",
    "\n",
    "    return mu_opt1, mu_opt2, mu_opt3, lambda_.value, b_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa36ad52",
   "metadata": {},
   "source": [
    "**!!!TODO: check b_opt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4b57f",
   "metadata": {},
   "source": [
    "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
    "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
    "    Use ``multiply`` for elementwise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "671cc261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_opt1 = [19.03475044]\n",
      "mu_opt2 = [0.72732141]\n",
      "mu_opt3 = [0.72502455]\n"
     ]
    }
   ],
   "source": [
    "def main_q4_b():\n",
    "    # \\rho = 2, p = 2, \\sigma = 0.5 and c = \\sum_{l=1}^3 \\mathrm{tr}(\\hat{K}^l)\n",
    "    data, labels = prepare_ionosphere_dataset()\n",
    "\n",
    "    x_tr, x_te, y_tr, y_te = train_test_split(data, labels, train_size=0.8)\n",
    "\n",
    "    # k_mats = compute_ks(x_tr, x_tr)\n",
    "    k_mats = compute_all_kmat(x_tr, x_tr)\n",
    "\n",
    "    res = kernel_learning(k_mats, y_tr, rho=2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_q4_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff54f9",
   "metadata": {},
   "source": [
    "### (c) Apply kernel trick for SVM prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c8a42",
   "metadata": {},
   "source": [
    "**(10 points)** Use the code skeletons `SVM_predict`(in `main.ipynb`) or `SVM_predict.m`.\n",
    "\n",
    "> The size of the dual QP is independent of the feature\n",
    "dimension D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2a7cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_predict(kernel, y_tr, y_te, lambda_opt, b_opt, rho):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        kernel: kernel matrix\n",
    "    Predict function for kernel SVM. \n",
    "    See lecture slide 183.\n",
    "    \"\"\"\n",
    "    n_te = len(y_te)\n",
    "    n_tr = len(y_tr)\n",
    "    \n",
    "    # wx = \\sum_{i=1}^{m} \\lambda_i y_i k_mat\n",
    "    result = b_opt\n",
    "    for idx, (lambda_i, y_i) in enumerate(zip(lambda_opt, y_tr)):\n",
    "        result += 1/rho * lambda_i * y_i * kernel[idx,:]\n",
    "    pred_y_te = np.sign(result)\n",
    "    acc = np.sum(y_te==pred_y_te)/n_te\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b94f6bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_opt1 = [18.66506083]\n",
      "mu_opt2 = [0.92886342]\n",
      "mu_opt3 = [0.92742913]\n",
      "Current accuracy is  0.3142857142857143\n"
     ]
    }
   ],
   "source": [
    "def main_q4_c():\n",
    "    # \\rho = 2, p = 2, \\sigma = 0.5 and c = \\sum_{l=1}^3 \\mathrm{tr}(\\hat{K}^l)\n",
    "    data, labels = prepare_ionosphere_dataset()\n",
    "    \n",
    "    x_tr, x_te, y_tr, y_te = train_test_split(data, labels, train_size=0.8, random_seed=3)\n",
    "    \n",
    "    kxx = compute_all_kmat(x_tr, x_tr)\n",
    "    \n",
    "    (mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt) = kernel_learning(kxx, y_tr, rho=7)\n",
    "    \n",
    "    kxx_primer = compute_all_kmat(x_tr, x_te)\n",
    "    \n",
    "    # kxx_primer_weighted: (281, 70)\n",
    "    kxx_primer_weighted = mu_opt1[0] * kxx_primer[0] + mu_opt2[0] * kxx_primer[1] + mu_opt3[0] * kxx_primer[2]\n",
    "    \n",
    "    acc = svm_predict(kxx_primer_weighted, y_tr, y_te, lambda_opt, b_opt, rho=2)\n",
    "    \n",
    "    print(\"Current accuracy is \", acc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_q4_c()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc905b",
   "metadata": {},
   "source": [
    "**TODO: Prediction accuracy very low!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028bf40",
   "metadata": {},
   "source": [
    "## B Repeat experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd9a3b",
   "metadata": {},
   "source": [
    "**(5 points)** Repeat the steps 4(a)–(c) 100 times with different seeds for the random partition of the data intotraining and test sets, and report the average test accuracy (correct classification rate) to Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71868e56",
   "metadata": {},
   "source": [
    "| Kernel function  | $\\hat{k}^1$ | $\\hat{k}^2$ | $\\hat{k}^3$ | $\\sum_{l=1}^3 \\hat{k}^l$ |\n",
    "| ---------------- | ----------- | ----------- | ----------- | ------------------------ |\n",
    "| Average accuracy |             |             |             |                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ca8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87f33c1a",
   "metadata": {},
   "source": [
    "## C Solve dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc4049",
   "metadata": {},
   "source": [
    "**(10  points)** For  each  of  the  100  training  and  test  sets  constructed  in  5.,  solve  (2)  using  the  kernels  functions $\\hat{k}^1$, $\\hat{k}^2$, and $\\hat{k}^3$, respectively, and report the average test accuracies in Table 1. Use the code skeletons `SVM_predict`(in `main.ipynb`) or `SVM_predict.m`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0a2cc",
   "metadata": {},
   "source": [
    "| Kernel function  | $\\hat{k}^1$ | $\\hat{k}^2$ | $\\hat{k}^3$ | $\\sum_{l=1}^3 \\hat{k}^l$ |\n",
    "| ---------------- | ----------- | ----------- | ----------- | ------------------------ |\n",
    "| Average accuracy |             |             |             |                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fe58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_fit(kernel, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Dual of soft-margin SVM problem (2)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "    n_tr = len(y_tr)\n",
    "    G =  ...\n",
    "    lambda_ = cp.Variable(n_tr)\n",
    "    dual_obj = cp.Maximize(... cp.quad_form(lambda_, psd_wrap(G)))\n",
    "    cons = []\n",
    "    ...\n",
    "    prob = cp.Problem(dual_obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "    lambda_opt = lambda_.value\n",
    "    b_opt =  ...\n",
    "    return lambda_opt, b_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_opt_kernel = []    \n",
    "acc_poly_kernel = []    \n",
    "acc_gauss_kernel = []    \n",
    "acc_linear_kernel = []    \n",
    "rho = 0.01\n",
    "# data, labels = prepare_ionosphere_dataset()\n",
    "for iters in range(100): \n",
    "    ## Please do not change the random seed.\n",
    "    np.random.seed(iters)\n",
    "    ### Training-test split\n",
    "    msk = np.random.rand(data_normalized.shape[0]) <=...\n",
    "    x_tr = data[...]\n",
    "    x_te = data[...]\n",
    "    y_tr = labels[...]\n",
    "    y_te = labels[...]\n",
    " \n",
    "    n_tr = y_tr.shape[0]\n",
    "    n_te = y_te.shape[0]\n",
    "    n_tr = x_tr.shape[0]\n",
    "    n_te = x_te.shape[0]\n",
    "    \n",
    "    x_all = np.vstack([x_tr, x_te])\n",
    "    n_all = x_all.shape[0]\n",
    "\n",
    "    ## Prepare the initial choice of kernels \n",
    "    # It is recommended to prepare the kernels for all the training and the test data\n",
    "    # Then, the kernel size will be (n_tr + n_te)x(n_tr + n_te).\n",
    "    # Use only the training block (like K1[0:n_tr, 0:n_tr] ) to learn the classifier \n",
    "    # (for the functions svm_fit and kernel_learning).\n",
    "    # When predicting you may use the whole kernel as it is. \n",
    "    K1 = ...\n",
    "    K2 = ...\n",
    "    K3 = ...\n",
    "\n",
    "    mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt = kernel_learning(...)\n",
    "    opt_kernel = ...\n",
    "    acc_opt_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_poly_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_gauss_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_linear_kernel.append(svm_predict(...)\n",
    "    print('Iteration-->' + str(iters))\n",
    "print('Average dual accuracy with optimal kernel is ' + str(np.mean(acc_opt_kernel)))\n",
    "print('Average dual accuracy with polynomial kernel is ' + str(np.mean(acc_poly_kernel)))\n",
    "print('Average dual accuracy with gaussian kernel is ' + str(np.mean(acc_gauss_kernel)))\n",
    "print('Average dual accuracy with linear kernel is ' + str(np.mean(acc_linear_kernel)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15debf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afb19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpy.atoms.affine.wraps import psd_wrap\n",
    "from read_data import *\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%       MGT - 418         %%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Convex Optimization - Project 2          %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%             2021-2022 Fall                    %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Learning the Kernel Function             %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f60e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_fit(kernel, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Dual of soft-margin SVM problem (2)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "    n_tr = len(y_tr)\n",
    "    G =  ...\n",
    "    lambda_ = cp.Variable(n_tr)\n",
    "    dual_obj = cp.Maximize(... cp.quad_form(lambda_, psd_wrap(G)))\n",
    "    cons = []\n",
    "    ...\n",
    "    prob = cp.Problem(dual_obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "    lambda_opt = lambda_.value\n",
    "    b_opt =  ...\n",
    "    return lambda_opt, b_opt\n",
    "\n",
    "\n",
    "def svm_predict(kernel, y_tr, y_te, lambda_opt, b_opt, rho):\n",
    "    \"\"\"\n",
    "    Predict function for kernel SVM. \n",
    "    See lecture slide 183.\n",
    "    \"\"\"\n",
    "    n_te = len(y_te)\n",
    "    n_tr = len(y_tr)\n",
    "    ...\n",
    "    acc = ...\n",
    "    return acc\n",
    "\n",
    "def kernel_learning(K1, K2, K3, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Kernel learning for soft margin SVM. \n",
    "    Implementation of problem (5)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "    ...\n",
    "    r1 = np.trace(K1) \n",
    "    ...\n",
    "    lambda_ = cp.Variable(n_tr)\n",
    "    z = cp.Variable(1)\n",
    "    ...\n",
    "    \n",
    "    cons = []\n",
    "    cons.append(z * r1 >= 1/ (2 * rho) * cp.quad_form(lambda_, psd_wrap(G1)))\n",
    "    ...\n",
    "    ...\n",
    "    prob = cp.Problem(obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "\n",
    "    mu_opt1 = cons[0].dual_value\n",
    "    ...\n",
    "    b_opt = ....dual_value\n",
    "    return mu_opt1, mu_opt2, mu_opt3, lambda_.value, b_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c9713b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_opt_kernel = []    \n",
    "acc_poly_kernel = []    \n",
    "acc_gauss_kernel = []    \n",
    "acc_linear_kernel = []    \n",
    "rho = 0.01\n",
    "# data, labels = prepare_ionosphere_dataset()\n",
    "for iters in range(100): \n",
    "    ## Please do not change the random seed.\n",
    "    np.random.seed(iters)\n",
    "    ### Training-test split\n",
    "    msk = np.random.rand(data_normalized.shape[0]) <=...\n",
    "    x_tr = data[...]\n",
    "    x_te = data[...]\n",
    "    y_tr = labels[...]\n",
    "    y_te = labels[...]\n",
    " \n",
    "    n_tr = y_tr.shape[0]\n",
    "    n_te = y_te.shape[0]\n",
    "    n_tr = x_tr.shape[0]\n",
    "    n_te = x_te.shape[0]\n",
    "    \n",
    "    x_all = np.vstack([x_tr, x_te])\n",
    "    n_all = x_all.shape[0]\n",
    "\n",
    "    ## Prepare the initial choice of kernels \n",
    "    # It is recommended to prepare the kernels for all the training and the test data\n",
    "    # Then, the kernel size will be (n_tr + n_te)x(n_tr + n_te).\n",
    "    # Use only the training block (like K1[0:n_tr, 0:n_tr] ) to learn the classifier \n",
    "    # (for the functions svm_fit and kernel_learning).\n",
    "    # When predicting you may use the whole kernel as it is. \n",
    "    K1 = ...\n",
    "    K2 = ...\n",
    "    K3 = ...\n",
    "\n",
    "    mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt = kernel_learning(...)\n",
    "    opt_kernel = ...\n",
    "    acc_opt_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_poly_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_gauss_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_linear_kernel.append(svm_predict(...)\n",
    "    print('Iteration-->' + str(iters))\n",
    "print('Average dual accuracy with optimal kernel is ' + str(np.mean(acc_opt_kernel)))\n",
    "print('Average dual accuracy with polynomial kernel is ' + str(np.mean(acc_poly_kernel)))\n",
    "print('Average dual accuracy with gaussian kernel is ' + str(np.mean(acc_gauss_kernel)))\n",
    "print('Average dual accuracy with linear kernel is ' + str(np.mean(acc_linear_kernel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c834cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
