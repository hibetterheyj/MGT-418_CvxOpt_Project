{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ad60ca",
   "metadata": {},
   "source": [
    "# Radar Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5145a2",
   "metadata": {},
   "source": [
    "## A Radar Classification Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a41617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpy.atoms.affine.wraps import psd_wrap\n",
    "from read_data import *\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%       MGT - 418         %%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Convex Optimization - Project 2          %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%             2021-2022 Fall                    %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Learning the Kernel Function             %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c9907",
   "metadata": {},
   "source": [
    "### (a) Read & Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc34d0",
   "metadata": {},
   "source": [
    "**(5 points)** Read the data file ionosphere.data into memory by using the scriptsreaddata.pyorreaddata.m.Use the code skeletonsmain.ipnyb or main.m to randomly select 80% of the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729b41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = prepare_ionosphere_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b795ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_data import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cabc1712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0\n",
      "[[1.0 0.97588 -0.10602 0.94601 -0.208 0.92806 -0.2835 0.85996 -0.27342\n",
      "  0.79766 -0.47929 0.78225 -0.50764 0.74628 -0.61436 0.57945 -0.68086\n",
      "  0.37852 -0.73641 0.36324 -0.76562 0.31898 -0.79753 0.22792 -0.81634\n",
      "  0.13659 -0.8251 0.04606 -0.82395 -0.04262 -0.81318 -0.13832 -0.80975]]\n",
      "(281, 33)\n",
      "#1\n",
      "[[1.0 -0.205 0.2875 0.23 0.1 0.2825 0.3175 0.3225 0.35 0.36285 -0.34617\n",
      "  0.0925 0.275 -0.095 0.21 -0.0875 0.235 -0.34187 0.31408 -0.48 -0.08\n",
      "  0.29908 0.33176 -0.58 -0.24 0.3219 -0.28475 -0.47 0.185 -0.27104\n",
      "  -0.31228 0.40445 0.0305]]\n",
      "(281, 33)\n",
      "#2\n",
      "[[1.0 1.0 0.5782 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0\n",
      "  1.0 -1.0 1.0 -1.0 1.0 -0.62796 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0 -1.0 1.0\n",
      "  -1.0 1.0 -1.0]]\n",
      "(281, 33)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"#{}\".format(i))\n",
    "    # data_train, data_test, labels_train, labels_test\n",
    "    data_train, _, _, _ = train_test_split(data, labels, train_size=0.8, random_seed=i)\n",
    "    print(data_train[:1,:])\n",
    "    print(np.shape(data_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a016f",
   "metadata": {},
   "source": [
    "### (b) Define kernel function & Solve QCQP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f054940",
   "metadata": {},
   "source": [
    "**(15 points)** Define polynomial, Gaussian, and linear kernel function, and construct the kernel matrices $\\hat{K}^l,\\ l = 1,2,3$, for all training samples\n",
    "Solve the QCQP in (5) for $\\rho = 2$, $p = 2$, $\\sigma = 2$ and $c = \\sum_{l=1}^3 \\mathrm{tr}(\\hat{K}^l) $ with CVXPY and MOSEK in Python or with YALMIP and GUROBI in MATLAB, and record the optimal dualvariables $\\mu_1^*$, $\\mu_2^*$, and $\\mu_3^*$. Use the code skeletons `kernel_learning` (in `main.ipynb`) or `kernel_learning.m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe76f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34ff54f9",
   "metadata": {},
   "source": [
    "### (c) Apply kernel trick for SVM prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c8a42",
   "metadata": {},
   "source": [
    "**(10 points)** Use the code skeletons `SVM_predict`(in `main.ipynb`) or `SVM_predict.m`.\n",
    "\n",
    "> The size of the dual QP is independent of the feature\n",
    "dimension D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7cc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b028bf40",
   "metadata": {},
   "source": [
    "## B Repeat experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd9a3b",
   "metadata": {},
   "source": [
    "**(5 points)** Repeat the steps 4(a)â€“(c) 100 times with different seeds for the random partition of the data intotraining and test sets, and report the average test accuracy (correct classification rate) to Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71868e56",
   "metadata": {},
   "source": [
    "| Kernel function  | $\\hat{k}^1$ | $\\hat{k}^2$ | $\\hat{k}^3$ | $\\sum_{l=1}^3 \\hat{k}^l$ |\n",
    "| ---------------- | ----------- | ----------- | ----------- | ------------------------ |\n",
    "| Average accuracy |             |             |             |                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ca8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87f33c1a",
   "metadata": {},
   "source": [
    "## C Solve dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc4049",
   "metadata": {},
   "source": [
    "**(10  points)** For  each  of  the  100  training  and  test  sets  constructed  in  5.,  solve  (2)  using  the  kernels  functions $\\hat{k}^1$, $\\hat{k}^2$, and $\\hat{k}^3$, respectively, and report the average test accuracies in Table 1. Use the code skeletons `SVM_predict`(in `main.ipynb`) or `SVM_predict.m`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0a2cc",
   "metadata": {},
   "source": [
    "| Kernel function  | $\\hat{k}^1$ | $\\hat{k}^2$ | $\\hat{k}^3$ | $\\sum_{l=1}^3 \\hat{k}^l$ |\n",
    "| ---------------- | ----------- | ----------- | ----------- | ------------------------ |\n",
    "| Average accuracy |             |             |             |                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e5e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b15debf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afb19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpy.atoms.affine.wraps import psd_wrap\n",
    "from read_data import *\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%       MGT - 418         %%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Convex Optimization - Project 2          %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%             2021-2022 Fall                    %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%      Learning the Kernel Function             %%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f60e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_fit(kernel, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Dual of soft-margin SVM problem (2)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "    n_tr = len(y_tr)\n",
    "    G =  ...\n",
    "    lambda_ = cp.Variable(n_tr)\n",
    "    dual_obj = cp.Maximize(... cp.quad_form(lambda_, psd_wrap(G)))\n",
    "    cons = []\n",
    "    ...\n",
    "    prob = cp.Problem(dual_obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "    lambda_opt = lambda_.value\n",
    "    b_opt =  ...\n",
    "    return lambda_opt, b_opt\n",
    "\n",
    "\n",
    "def svm_predict(kernel, y_tr, y_te, lambda_opt, b_opt, rho):\n",
    "    \"\"\"\n",
    "    Predict function for kernel SVM. \n",
    "    See lecture slide 183.\n",
    "    \"\"\"\n",
    "    n_te = len(y_te)\n",
    "    n_tr = len(y_tr)\n",
    "    ...\n",
    "    acc = ...\n",
    "    return acc\n",
    "\n",
    "def kernel_learning(K1, K2, K3, y_tr, rho):\n",
    "    \"\"\"\n",
    "    Kernel learning for soft margin SVM. \n",
    "    Implementation of problem (5)\n",
    "    Use cvxpy.atoms.affine.psd_wrap for each G(\\hat K^l) matrix when it appear in the constraints and in the objective\n",
    "    \"\"\"\n",
    "    ...\n",
    "    r1 = np.trace(K1) \n",
    "    ...\n",
    "    lambda_ = cp.Variable(n_tr)\n",
    "    z = cp.Variable(1)\n",
    "    ...\n",
    "    \n",
    "    cons = []\n",
    "    cons.append(z * r1 >= 1/ (2 * rho) * cp.quad_form(lambda_, psd_wrap(G1)))\n",
    "    ...\n",
    "    ...\n",
    "    prob = cp.Problem(obj, cons)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "\n",
    "    mu_opt1 = cons[0].dual_value\n",
    "    ...\n",
    "    b_opt = ....dual_value\n",
    "    return mu_opt1, mu_opt2, mu_opt3, lambda_.value, b_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c9713b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_opt_kernel = []    \n",
    "acc_poly_kernel = []    \n",
    "acc_gauss_kernel = []    \n",
    "acc_linear_kernel = []    \n",
    "rho = 0.01\n",
    "# data, labels = prepare_ionosphere_dataset()\n",
    "for iters in range(100): \n",
    "    ## Please do not change the random seed.\n",
    "    np.random.seed(iters)\n",
    "    ### Training-test split\n",
    "    msk = np.random.rand(data_normalized.shape[0]) <=...\n",
    "    x_tr = data[...]\n",
    "    x_te = data[...]\n",
    "    y_tr = labels[...]\n",
    "    y_te = labels[...]\n",
    " \n",
    "    n_tr = y_tr.shape[0]\n",
    "    n_te = y_te.shape[0]\n",
    "    n_tr = x_tr.shape[0]\n",
    "    n_te = x_te.shape[0]\n",
    "    \n",
    "    x_all = np.vstack([x_tr, x_te])\n",
    "    n_all = x_all.shape[0]\n",
    "\n",
    "    ## Prepare the initial choice of kernels \n",
    "    # It is recommended to prepare the kernels for all the training and the test data\n",
    "    # Then, the kernel size will be (n_tr + n_te)x(n_tr + n_te).\n",
    "    # Use only the training block (like K1[0:n_tr, 0:n_tr] ) to learn the classifier \n",
    "    # (for the functions svm_fit and kernel_learning).\n",
    "    # When predicting you may use the whole kernel as it is. \n",
    "    K1 = ...\n",
    "    K2 = ...\n",
    "    K3 = ...\n",
    "\n",
    "    mu_opt1, mu_opt2, mu_opt3, lambda_opt, b_opt = kernel_learning(...)\n",
    "    opt_kernel = ...\n",
    "    acc_opt_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_poly_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_gauss_kernel.append(svm_predict(...))\n",
    "    \n",
    "    lambda_opt, b_opt = svm_fit(...)\n",
    "    acc_linear_kernel.append(svm_predict(...)\n",
    "    print('Iteration-->' + str(iters))\n",
    "print('Average dual accuracy with optimal kernel is ' + str(np.mean(acc_opt_kernel)))\n",
    "print('Average dual accuracy with polynomial kernel is ' + str(np.mean(acc_poly_kernel)))\n",
    "print('Average dual accuracy with gaussian kernel is ' + str(np.mean(acc_gauss_kernel)))\n",
    "print('Average dual accuracy with linear kernel is ' + str(np.mean(acc_linear_kernel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c834cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
